{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "akk_word2vec.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YGmKj--mpixYHu00U2AHq8XbOUwQXIMf",
      "authorship_tag": "ABX9TyMsBKL33ybQiWcGcuZUpH1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mstekel/akk_word2vec/blob/main/akk_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzIO79f7Rmgt"
      },
      "source": [
        "import glob\r\n",
        "for file_path in glob.glob('drive/MyDrive/datasets/oracc/*.xml'):\r\n",
        "  print(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v3wToDlhGhE"
      },
      "source": [
        "!export PYTHONHASHSEED=777\n",
        "\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils, models\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "window = 2\n",
        "model_path = f'drive/MyDrive/models/word2vec/oracc_lemmas_{window}.model'\n",
        "load_model_from_drive = True\n",
        "\n",
        "class OraccCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "    \n",
        "    def __iter__(self):\n",
        "      for file_path in glob.glob('drive/MyDrive/datasets/oracc/*.xml'):\n",
        "        t = ET.parse(file_path)\n",
        "        namespaces = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
        "        for s in t.findall('.//tei:s', namespaces):\n",
        "          lemmatized_sentence = []\n",
        "          for w in s.findall('.//tei:w', namespaces):\n",
        "            #sentence.append(' '.join(w.itertext()))\n",
        "            lemma = w.get('lemma')\n",
        "            if lemma != None:\n",
        "              lemma,_,_ = lemma.rpartition('[')\n",
        "              lemmatized_sentence.append(lemma)\n",
        "            else:\n",
        "              lemmatized_sentence.append('')    \n",
        "          yield s, lemmatized_sentence\n",
        "\n",
        "sentences = OraccCorpus()\n",
        "\n",
        "if(load_model_from_drive):\n",
        "  model = models.Word2Vec.load(model_path)\n",
        "else:\n",
        "  print(f\"Model build started: {datetime.now().time()}\")\n",
        "  for i, s in enumerate(sentences):\n",
        "    if i == 10:\n",
        "      break\n",
        "    print(i, s)\n",
        "\n",
        "  model = models.Word2Vec(sentences=sentences, min_count=1, workers=1, size=300, window=window)\n",
        "  model.save(model_path)\n",
        "  print(f\"Model build finished: {datetime.now().time()}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrDD2GOV-2so"
      },
      "source": [
        "from html import escape\n",
        "\n",
        "term = 'galû'\n",
        "term_lemmas = [\n",
        "    # 'šuglû[deported//deported]AJ', \n",
        "    # 'galû[be(come) deported//be(come) deported]V', \n",
        "    # 'galû[be(come) deported//emigrate]V',\n",
        "    # 'galû[be(come) deported//smuggle]V',\n",
        "    # 'galû[be(come) deported//exile]V', \n",
        "    # 'galû[be(come) deported//send into exile]V',\n",
        "    # 'galû[be(come) deported//deport]V', \n",
        "    # 'galû[be deported//take/send into exile]V', \n",
        "    # 'šagalûtu[deportation//deportation]N'\n",
        "    term,\n",
        "    #'šuglû',\n",
        "    #'šagalûtu'\n",
        "]\n",
        "\n",
        "term_span = []\n",
        "for lemma in term_lemmas:\n",
        "  term_span = term_span + model.wv.most_similar(positive=[lemma], topn=10)\n",
        "term_span"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbtO8qHuYoGH"
      },
      "source": [
        "term_sentences = [sentence for sentence in sentences if any(lemma in sentence[1] for lemma in term_lemmas)]\r\n",
        "print(f'{len(term_sentences)} sentences:')\r\n",
        "term_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Utz0irco4m"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "values = []\r\n",
        "for s in term_sentences:\r\n",
        "  sv = np.sum(np.array([model.wv[w] for w in s[1]]), axis=0) \r\n",
        "  values.append(sv)\r\n",
        "values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO-FAIb3WOA1"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "tv = model.wv[term]\r\n",
        "values = []\r\n",
        "for s in term_sentences:\r\n",
        "  sv = np.mean(np.array([np.dot(model.wv[w], tv) for w in s[1]])) \r\n",
        "  values.append([sv])\r\n",
        "values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B6pHXtlkEfB"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.signal import find_peaks\n",
        "from termcolor import colored\n",
        "import json\n",
        "\n",
        "def find_optimal_k(values, max_k=10):\n",
        "    sil = [(1, 0)]\n",
        "    for k in range(2, min(len(values) - 1, max_k)):\n",
        "        kmeans = KMeans(n_clusters=k).fit(values)\n",
        "        sil.append((k, silhouette_score(values, kmeans.labels_, metric='euclidean')))\n",
        "    k_values, sil_values = zip(*sil)\n",
        "    peaks = find_peaks(sil_values)\n",
        "    if len(peaks[0]) > 0:\n",
        "        return k_values[peaks[0][0]]\n",
        "    else:\n",
        "        max_sil_value = max(sil_values)\n",
        "        if max_sil_value > 0:\n",
        "            k_value = k_values[sil_values.index(max_sil_value)]\n",
        "        else:\n",
        "            k_value = 1\n",
        "        return k_value\n",
        "\n",
        "opt_k = find_optimal_k(values)\n",
        "\n",
        "decode_sentence = lambda s: \"\".join(s).replace(\"\\n\", \" \")\n",
        "clustering = KMeans(n_clusters=opt_k).fit(values)\n",
        "cluster_bests, _ = pairwise_distances_argmin_min(clustering.cluster_centers_, values)\n",
        "result = {\n",
        "        'clusters': [\n",
        "            {\n",
        "                'cluster': str(l),\n",
        "                'best': decode_sentence(term_sentences[cluster_bests[l]][0].itertext()),\n",
        "                'all': [\n",
        "                    f'{decode_sentence(s[0].itertext())}'\n",
        "                    for i, s in enumerate(term_sentences)\n",
        "                    if clustering.labels_[i] == l\n",
        "                ]\n",
        "            }\n",
        "            for l in set(clustering.labels_)\n",
        "        ]\n",
        "    }\n",
        "print(colored(json.dumps(result, indent=2, ensure_ascii=False), 'red'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}